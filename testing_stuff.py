
from decisionTreeClassifier import DecisionTreeClassifier
from randomForestClassifier import RandomForestClassifier
from dataset import Dataset

import matplotlib.pyplot as plt
import numpy as np

from scikitClassifier import ScikitClassifier


def plot_iris_decision_tree(data_set):
    """
    ================================================================
    Plot the decision surface of a decision tree on the iris dataset
    ================================================================

    Plot the decision surface of a decision tree trained on pairs
    of features of the iris dataset.

    See :ref:`decision tree <tree>` for more information on the estimator.

    For each pair of iris features, the decision tree learns decision
    boundaries made of combinations of simple thresholding rules inferred from
    the training samples.
    """
    print(__doc__)

    # Parameters
    n_classes = 2 # malware and benign
    plot_colors = "rb"
    plot_step = 0.02

    # Load data
    #iris = load_iris()

    for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],
                                    [0, 4], [0, 5], [1, 2],
                                    [1, 3], [1, 4], [1, 5],
                                    [2, 3], [2, 4], [2, 5],
                                    [3, 4], [3, 5], [4, 5]]):  # match each feature with each other
        # We only take the two corresponding features
        X = data_set.features[:pair]
        y = data_set.labels

        # Train
        clf = DecisionTreeClassifier().fit(X, y)

        # Plot the decision boundary
        plt.subplot(2, 3, pairidx + 1)  # ???

        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                             np.arange(y_min, y_max, plot_step))
        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

        plt.xlabel(data_set.feature_names[pair[0]])
        plt.ylabel(data_set.feature_names[pair[1]])

        # Plot the training points
        for i, color in zip(range(n_classes), plot_colors):
            idx = np.where(y == i)
            plt.scatter(X[idx, 0], X[idx, 1], c=color, label=data_set.label_names[i],
                        cmap=plt.cm.RdYlBu, edgecolor='black', s=15)

    plt.suptitle("Decision surface of a decision tree using paired features")
    plt.legend(loc='lower right', borderpad=0, handletextpad=0)
    plt.axis("tight")
    plt.show()


data = Dataset()
data.load_data(0.3)
#plot_iris_decision_tree(data)

training_data, testing_data, training_labels, testing_labels = data.partition_data(0.3)

decision_tree = DecisionTreeClassifier()

decision_tree.fit(training_data, training_labels)
decision_tree.predict(testing_data)
decision_tree.create_confusion_matrix(testing_labels, data.label_names)
decision_tree.calculate_accuracy(testing_data, testing_labels)
decision_tree.dysplay_confusion_matrix(testing_labels, decision_tree.predicted_labels, data.label_names)

random_forest = RandomForestClassifier()

random_forest.fit(training_data, training_labels)
random_forest.predict(testing_data)
random_forest.create_confusion_matrix(testing_labels, data.label_names)
random_forest.calculate_accuracy(testing_data, testing_labels)
random_forest.dysplay_confusion_matrix(testing_labels, random_forest.predicted_labels, data.label_names)