#
#"""
#
#"""
#
#from decisionTreeClassifier import DecisionTreeClassifier
#from randomForestClassifier import RandomForestClassifier
#from dataset import Dataset
#
#import matplotlib.pyplot as plt
#import numpy as np
#
#from scikitClassifier import ScikitClassifier
#
#
#def plot_iris_decision_tree(data_set):
#    """
#    ================================================================
#    Plot the decision surface of a decision tree on the iris dataset
#    ================================================================
#
#    Plot the decision surface of a decision tree trained on pairs
#    of features of the iris dataset.
#
#    See :ref:`decision tree <tree>` for more information on the estimator.
#
#    For each pair of iris features, the decision tree learns decision
#    boundaries made of combinations of simple thresholding rules inferred from
#    the training samples.
#    """
#    print(__doc__)
#
#    # Parameters
#    n_classes = 2 # malware and benign
#    plot_colors = "rb"
#    plot_step = 0.02
#
#    # Load data
#    #iris = load_iris()
#
#    for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],
#                                    [0, 4], [0, 5], [1, 2],
#                                    [1, 3], [1, 4], [1, 5],
#                                    [2, 3], [2, 4], [2, 5],
#                                    [3, 4], [3, 5], [4, 5]]):  # match each feature with each other
#        # We only take the two corresponding features
#        X = data_set.features[:pair]
#        y = data_set.labels
#
#        # Train
#        clf = DecisionTreeClassifier().fit(X, y)
#
#        # Plot the decision boundary
#        plt.subplot(2, 3, pairidx + 1)  # ???
#
#        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
#        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
#                             np.arange(y_min, y_max, plot_step))
#        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
#
#        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
#        Z = Z.reshape(xx.shape)
#        cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)
#
#        plt.xlabel(data_set.feature_names[pair[0]])
#        plt.ylabel(data_set.feature_names[pair[1]])
#
#        # Plot the training points
#        for i, color in zip(range(n_classes), plot_colors):
#            idx = np.where(y == i)
#            plt.scatter(X[idx, 0], X[idx, 1], c=color, label=data_set.label_names[i],
#                        cmap=plt.cm.RdYlBu, edgecolor='black', s=15)
#
#    plt.suptitle("Decision surface of a decision tree using paired features")
#    plt.legend(loc='lower right', borderpad=0, handletextpad=0)
#    plt.axis("tight")
#    plt.show()
#
#
#data = Dataset()
#data.load_data(0.3)
###plot_iris_decision_tree(data)
##
#training_data, testing_data, training_labels, testing_labels = data.partition_data(0.3)
##
#decision_tree = DecisionTreeClassifier()
##
#for i in range(2):
#    training_data, testing_data, training_labels, testing_labels = data.partition_data(0.3)
#    decision_tree.fit(training_data, training_labels)
#    decision_tree.export_tree_as_pdf(data.feature_names, data.label_names) # 5, 6
##decision_tree.predict(testing_data)
##decision_tree.create_confusion_matrix(testing_labels, data.label_names)
##decision_tree.calculate_accuracy(testing_data, testing_labels)
##decision_tree.dysplay_confusion_matrix(testing_labels, decision_tree.predicted_labels, data.label_names)
##
##random_forest = RandomForestClassifier()
##
##random_forest.fit(training_data, training_labels)
##random_forest.predict(testing_data)
##random_forest.create_confusion_matrix(testing_labels, data.label_names)
##random_forest.calculate_accuracy(testing_data, testing_labels)
##random_forest.dysplay_confusion_matrix(testing_labels, random_forest.predicted_labels, data.label_names)
#
##def ret_test():
##    return
##
##def null_test():
##    return None
##
##print(ret_test())
##print(null_test())
#
##count = [0, 0]
##
##str_arr = ["True", "False", "True"]
##
##for i in str_arr:
##    count[i == "True"] += 1
##
##print(count)
#
#
#import subprocess
#
#print("start")
#subprocess.call("./chilp.sh")
#print("end")
#
#
#def a(p1, p2, p3, p4, p5):
#    print(f"{p1}, {p2}, {p3}, {p4}, {p5}")
#
#
#def b(p1, p2):
#    print(f"{p1}, {p2}")
#
#
#def c():
#    print("No param")
#
#
#letter_dict = {
#    "a": a,
#    "b": b,
#    "c": c
#}
#
#
#def switch_func(param, *args):
#    try:
#        letter_dict[param](*args)
#    except TypeError:
#        print(f"TypeError for {param} too many args {len(args)}")
#    except KeyError:
#        print("No func!")
#
#
#bla = [1, 2]
#switch_func("b", *bla)
#
#switch_func("a", 1, 2, 3, 4, 5)
#switch_func("b", 1, 2, 3, 4, 5)
#switch_func("a", 1, 2)
#switch_func("b", 1, 2)
#switch_func("c")
#
#
#test_dict = {
#    "one": 1,
#    "two": 2
#}
#
#print(test_dict["one"])
#print(test_dict[1])
#
#
# def method_to_test(x, y):
#     return x + y
#
#
# temp_list = []
#
# if temp_list:
#     print("ok")
#
#
#import machineLearningFramework as mlf
#
#print(mlf.CLASSIFIER)
#
#mlf.select_classifier(123)
#
#print(mlf.CLASSIFIER)
#import machineLearningFramework as mlf
#
#malware_rate = 0.33
#partition_size = 0.33
#
#mlf.load_json_data(malware_rate)
#
#mlf.partition_data(partition_size)
#
#for key, item in mlf.PARTITIONED_DATA_DICT.items():
#    print(item)
#
#print(mlf.CLASSIFIER)
#mlf.select_classifier("Decision Tree")
#print(mlf.CLASSIFIER)
#
#mlf.train_classifier()
#
#ret_val = mlf.classify_new_data()
#print(ret_val)
#
#mlf.display_confusion_matrix()

#from tests import mammals

#mammal = mammals.Mammals()

#mammal.print_a()
